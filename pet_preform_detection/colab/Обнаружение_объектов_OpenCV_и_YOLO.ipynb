{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73-T-LgF0rc"
      },
      "source": [
        "#Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ·Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ OpenCV\n",
        "1) ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸\n",
        "2) ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğµ\n",
        "3) Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ñ…Ğ¾Ğ´ Ñ Ğ²ĞµĞ±-ĞºĞ°Ğ¼ĞµÑ€Ñ‹.\n",
        "4) Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\n",
        "5) ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±, ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¸ Ğ¼ĞµĞ½ÑĞµÑ‚ RB.\n",
        "6) ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ†Ğ¸ĞºĞ», Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ²ĞµĞ±-ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ, Ñ€Ğ¸ÑÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².\n",
        "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ÑÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ cv2_imshow().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YxCf_FY6NFo",
        "outputId": "21fa93ab-0d61-481e-a4ed-fcb5b0bf5b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYSnOOmkMc44"
      },
      "outputs": [],
      "source": [
        "# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ - Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ\n",
        "#!pip install opencv-python tensorflow numpy imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00cvcZQ37MTb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOhgatreY9kB"
      },
      "source": [
        " Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ·Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ OpenCV Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, SSD MobileNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-ZmKvU4MFVEM"
      },
      "outputs": [],
      "source": [
        "# @title ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°Ğ¼\n",
        "# Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸\n",
        "path_video = '/content/drive/MyDrive/track001.mp4'\n",
        "# Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "vidcap = cv2.VideoCapture(path_video)\n",
        "length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(length)\n",
        "# Ğ¡Ñ‡ĞµÑ‚Ñ‡Ğ¸Ğº ĞºĞ°Ğ´Ñ€Ğ¾Ğ²\n",
        "frame_count = 0\n",
        "for i in range(length):\n",
        "  if i == i:\n",
        "  # ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ„Ñ€ĞµĞ¹Ğ¼\n",
        "    hasFrame, image = vidcap.read()\n",
        "    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "    filename = f'/content/drive/MyDrive/dataset/train/background/frame_{frame_count}.jpg'\n",
        "    # Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ¼ BGR Ğ² RGB\n",
        "    color = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ°Ğ´Ñ€\n",
        "    success = cv2.imwrite(filename, image)\n",
        "    # Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€\n",
        "    plt.imshow(color)\n",
        "    plt.show()\n",
        "  frame_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bR9e-u1fRv_p",
        "outputId": "5feea95a-a926-4c17-bf4b-aa2822e9c279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 276 images belonging to 2 classes.\n",
            "Found 15 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m 9/31\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.5853 - loss: 1.4203"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 777ms/step - accuracy: 0.6045 - loss: 1.3150 - val_accuracy: 0.6667 - val_loss: 0.6458\n",
            "Epoch 2/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 326ms/step - accuracy: 0.7125 - loss: 0.6492 - val_accuracy: 0.6667 - val_loss: 0.6663\n",
            "Epoch 3/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 316ms/step - accuracy: 0.7011 - loss: 0.5872 - val_accuracy: 0.6667 - val_loss: 0.6194\n",
            "Epoch 4/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 400ms/step - accuracy: 0.7039 - loss: 0.5865 - val_accuracy: 0.6667 - val_loss: 0.6139\n",
            "Epoch 5/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 328ms/step - accuracy: 0.7053 - loss: 0.5634 - val_accuracy: 0.6667 - val_loss: 0.6182\n",
            "Epoch 6/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 313ms/step - accuracy: 0.7197 - loss: 0.5405 - val_accuracy: 0.6667 - val_loss: 0.7307\n",
            "Epoch 7/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 388ms/step - accuracy: 0.7356 - loss: 0.5119 - val_accuracy: 0.7333 - val_loss: 0.5663\n",
            "Epoch 8/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 304ms/step - accuracy: 0.7836 - loss: 0.4736 - val_accuracy: 0.6667 - val_loss: 0.6517\n",
            "Epoch 9/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 386ms/step - accuracy: 0.8052 - loss: 0.4196 - val_accuracy: 0.6667 - val_loss: 0.5845\n",
            "Epoch 10/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 374ms/step - accuracy: 0.8229 - loss: 0.4213 - val_accuracy: 0.7333 - val_loss: 0.6339\n",
            "Epoch 11/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 328ms/step - accuracy: 0.8098 - loss: 0.4273 - val_accuracy: 0.6667 - val_loss: 0.7675\n",
            "Epoch 12/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 307ms/step - accuracy: 0.7695 - loss: 0.4526 - val_accuracy: 0.8000 - val_loss: 0.7173\n",
            "Epoch 13/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 375ms/step - accuracy: 0.8348 - loss: 0.3545 - val_accuracy: 0.7333 - val_loss: 0.6404\n",
            "Epoch 14/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 332ms/step - accuracy: 0.8659 - loss: 0.3119 - val_accuracy: 0.6000 - val_loss: 0.6622\n",
            "Epoch 15/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 375ms/step - accuracy: 0.8302 - loss: 0.3957 - val_accuracy: 0.7333 - val_loss: 0.5970\n",
            "Epoch 16/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 370ms/step - accuracy: 0.8719 - loss: 0.3543 - val_accuracy: 0.8000 - val_loss: 0.7467\n",
            "Epoch 17/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 371ms/step - accuracy: 0.8462 - loss: 0.3191 - val_accuracy: 0.6667 - val_loss: 0.5887\n",
            "Epoch 18/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 306ms/step - accuracy: 0.8655 - loss: 0.3220 - val_accuracy: 0.7333 - val_loss: 0.6153\n",
            "Epoch 19/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 371ms/step - accuracy: 0.8756 - loss: 0.3173 - val_accuracy: 0.7333 - val_loss: 0.6493\n",
            "Epoch 20/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 400ms/step - accuracy: 0.8924 - loss: 0.2970 - val_accuracy: 0.7333 - val_loss: 0.6410\n",
            "Epoch 21/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 374ms/step - accuracy: 0.8679 - loss: 0.3101 - val_accuracy: 0.6000 - val_loss: 0.7497\n",
            "Epoch 22/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 303ms/step - accuracy: 0.8956 - loss: 0.2721 - val_accuracy: 0.8000 - val_loss: 0.6983\n",
            "Epoch 23/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 376ms/step - accuracy: 0.8814 - loss: 0.2920 - val_accuracy: 0.7333 - val_loss: 0.6498\n",
            "Epoch 24/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 350ms/step - accuracy: 0.8761 - loss: 0.2880 - val_accuracy: 0.6667 - val_loss: 0.8099\n",
            "Epoch 25/25\n",
            "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 376ms/step - accuracy: 0.8623 - loss: 0.2906 - val_accuracy: 0.6000 - val_loss: 0.5738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# @title ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\n",
        "img_width, img_height = 640, 352\n",
        "train_data_dir = '/content/drive/MyDrive/dataset/train/'\n",
        "validation_data_dir = '/content/drive/MyDrive/dataset/test'\n",
        "nb_train_samples = 1000\n",
        "nb_validation_samples = 200\n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "\n",
        "# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size)\n",
        "\n",
        "# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "model.save('object_detection_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8gZPQZub0lIJ"
      },
      "outputs": [],
      "source": [
        "#@title Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°Ğ¹Ğ»\n",
        "def save_box(ROI_number, i, original, x, y, h, w):\n",
        "  x=x-5\n",
        "  y=y-5\n",
        "  h=h+5\n",
        "  w=w+5\n",
        "  ROI = original[y:y+h, x:x+w]\n",
        "  if h>w: ROI =cv2.rotate(ROI, cv2.ROTATE_90_CLOCKWISE)\n",
        "  cv2.imwrite(f\"/content/drive/MyDrive/box/ROI_{ROI_number}_i_{i}.png\", ROI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JwbOhcdMc6dz"
      },
      "outputs": [],
      "source": [
        "#Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸\n",
        "CONFIDENCE_THRESHOLD = 0.6\n",
        "INPUT_WIDTH = 640\n",
        "INPUT_HEIGHT = 352\n",
        "# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹\n",
        "#cap = cv2.VideoCapture(0)  # 0 - Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ°\n",
        "path_video = '/content/drive/MyDrive/track001.mp4'\n",
        "# Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ½Ğ° Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ°Ğ¹Ğ»Ñƒ - Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "cap = cv2.VideoCapture(path_video)\n",
        "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "# Ğ¦Ğ²ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸\n",
        "colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0)]\n",
        "ROI_number = 0\n",
        "for i in range(length):\n",
        "#while True:\n",
        "    # Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ° Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² ĞºĞ°Ğ´Ñ€Ğ°\n",
        "    (H, W) = frame.shape[:2]\n",
        "    # ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "    # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ\n",
        "    image = cv2.resize(frame, (INPUT_WIDTH, INPUT_HEIGHT))\n",
        "    image = image / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    # ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° FPS\n",
        "    start_time = time.time()\n",
        "    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹\n",
        "    if 1==1:\n",
        "        # ĞŸĞ¾Ğ¸ÑĞº ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ CLAHE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ°\n",
        "        #clahe = cv2.createCLAHE(clipLimit=15, tileGridSize=(8,8))\n",
        "        # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ CLAHE Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "        #enhanced = clahe.apply(blurred)\n",
        "\n",
        "        #Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (0 - Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ) Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "        #ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ„Ğ»Ğ°Ğ³Ğ¾Ğ²: cv2.THRESH_BINARY_INV â€” Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³, cv2.THRESH_OTSU â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°\n",
        "        thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "        # ĞœĞ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ ÑÑ€Ğ¾Ğ·Ğ¸Ñ\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
        "        #Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° ĞŸĞ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° ÑÑ€Ğ¾Ğ·Ğ¸Ñ ->Ğ´Ğ¸Ğ»Ğ°Ñ‚Ğ°Ñ†Ğ¸Ñ\n",
        "        #ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ iterations=1 Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\n",
        "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "        #Ğ´Ğ¸Ğ»Ğ°Ñ‚Ğ°Ñ†Ğ¸Ñ->ÑÑ€Ğ¾Ğ·Ğ¸Ñ Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ€ÑÑ‚Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²\n",
        "        #Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ iterations=2 Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\n",
        "        closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "        colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0)]\n",
        "\n",
        "        # ĞŸĞ¾Ğ¸ÑĞº ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ² cv2.CHAIN_APPROX_SIMPLE â€” ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹\n",
        "        # cv2.RETR_EXTERNAL - Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ²\n",
        "        #Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹)\n",
        "        contours, _ = cv2.findContours(closing, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        for contour in contours:\n",
        "        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸\n",
        "          if 600<cv2.contourArea(contour)<2800:\n",
        "             x, y, w, h = cv2.boundingRect(contour)\n",
        "             #Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ x Ğ¸ y\n",
        "             if 125<y<260 and (20<x<220 or 270<x<550):\n",
        "               save_box(ROI_number, i, frame.copy(), x, y, h, w)\n",
        "               # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°\n",
        "               ROI_number += 1\n",
        "\n",
        "        # ĞÑ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ° ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ²\n",
        "        for contour in contours:\n",
        "        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸\n",
        "          if 600<cv2.contourArea(contour)<2800:\n",
        "             x, y, w, h = cv2.boundingRect(contour)\n",
        "             #Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ x Ğ¸ y\n",
        "             if 125<y<260 and (20<x<220 or 270<x<550):\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                # Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸\n",
        "                cv2.putText(frame, f\"X: {x}, Y: {y}\", (x, y - 10),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[0], 2)\n",
        "                M = cv2.moments(contour)\n",
        "                cx = int(M['m10'] / M['m00'])\n",
        "                cy = int(M['m01'] / M['m00'])\n",
        "                cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
        "    #Ğ Ğ°ÑÑ‡ĞµÑ‚ FPS Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²\n",
        "    #end_time = time.time()\n",
        "    #fps = 1.0 / (end_time - start_time)\n",
        "    #cv2.putText(frame, f\"FPS: {fps:.2f} i: {i:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    # ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°\n",
        "    #cv2_imshow(frame)\n",
        "    #cv2.imwrite(f\"/content/drive/MyDrive/frame/frame_{i}.png\", frame)\n",
        "    # Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ Ğ½Ğ°Ğ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ»Ğ°Ğ²Ğ¸ÑˆĞ¸ 'q'\n",
        "    #if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
        "\n",
        "# ĞÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²\n",
        "cap.release()\n",
        "#cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_invKbI6nK9C"
      },
      "source": [
        "# Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ·Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ YOLO\n",
        "1. Ğ ĞµĞ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ BoundigBox Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ² labelstudio Ğ¸ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ images + labels\n",
        "2. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ YOLO Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ° Ğ¸Ğ· Ğ²ĞµÑ€ÑĞ¸Ğ¸ v8 Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ yolov8n. Ğ¢Ğ°Ğº Ğ¶Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (yolov8n â†’ yolov8m â†’ yolov8l). ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ¼ĞµĞ½ĞµĞµ 200. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ 50.\n",
        "3. ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (yolov8_custom.yaml):\n",
        "\n",
        "train: ./data/train/images\n",
        "\n",
        "val: ./data/test/images\n",
        "\n",
        "nc: 1  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¾Ğ²\n",
        "\n",
        "names: ['preform']  # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL6V4amRnUdF",
        "outputId": "ab5f16c5-898f-46f5-e39d-1c8608e2a52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.218-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Downloading ultralytics-8.3.218-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.218 ultralytics-thop-2.0.17\n"
          ]
        }
      ],
      "source": [
        "# @title Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹\n",
        "!pip install ultralytics opencv-python numpy torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKEudshNhdZB",
        "outputId": "574370f1-5537-4958-9ffa-ace69baec3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/drive/MyDrive/custom_yolo/runs/yolov8_custom.yaml\n"
          ]
        }
      ],
      "source": [
        "# @title ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° yaml\n",
        "%%writefile /content/drive/MyDrive/custom_yolo/runs/yolov8_custom.yaml\n",
        "train: ./data/train/images\n",
        "val: ./data/test/images\n",
        "\n",
        "nc: 1 # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¾Ğ²\n",
        "names: ['preform'] # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlKfBH8cnPy2",
        "outputId": "5919ba40-f70a-4726-fb4c-6ca408fd88ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.218 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/custom_yolo/runs/yolov8_custom.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=[640, 352], int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/exp, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 3.0MB/s 0.2s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 229.9MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "WARNING âš ï¸ updating to 'imgsz=640'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.6Â±0.2 ms, read: 0.1Â±0.0 MB/s, size: 59.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/custom_yolo/runs/data/train/labels.cache... 28 images, 6 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 28/28 41.3Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.6Â±0.2 ms, read: 0.1Â±0.0 MB/s, size: 61.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/custom_yolo/runs/data/test/labels.cache... 15 images, 6 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 15/15 16.0Kit/s 0.0s\n",
            "Plotting labels to /content/runs/exp/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/20      1.97G      1.729      4.126      1.381         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 0.2it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.9it/s 1.2s\n",
            "                   all         15         18    0.00289      0.722    0.00252    0.00147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/20      1.97G       1.74      4.228      1.382         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.3it/s 0.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.0it/s 0.2s\n",
            "                   all         15         18    0.00289      0.722    0.00257    0.00149\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/20      1.97G      1.679      4.384      1.369         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.3it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.2it/s 0.1s\n",
            "                   all         15         18    0.00311      0.778    0.00295    0.00155\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/20      1.99G      1.603      4.055      1.298         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.5it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.9it/s 0.1s\n",
            "                   all         15         18    0.00378      0.944    0.00823    0.00501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/20         2G      1.223      3.526      1.112         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.2it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.5it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0127    0.00756\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/20      2.02G       1.07      2.787      1.034         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.5it/s 0.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.6it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0139    0.00852\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/20      2.02G      1.123      2.062      1.036         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.9it/s 0.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.8it/s 0.3s\n",
            "                   all         15         18      0.004          1     0.0135    0.00819\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/20      2.04G      1.203      2.481      1.123         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 3.7it/s 0.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.3it/s 0.2s\n",
            "                   all         15         18      0.004          1     0.0133    0.00785\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/20      2.04G      1.267      2.184      1.078         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.2it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.1it/s 0.2s\n",
            "                   all         15         18      0.004          1     0.0129    0.00762\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/20      2.05G       1.15      1.786      1.039         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.1it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.8it/s 0.2s\n",
            "                   all         15         18      0.004          1     0.0123    0.00707\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/20      2.06G      1.081      2.692      1.053          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 2.0it/s 1.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.4it/s 0.1s\n",
            "                   all         15         18      0.004          1      0.012     0.0069\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/20      2.08G     0.9914      2.503     0.9638         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.3it/s 0.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.8it/s 0.4s\n",
            "                   all         15         18      0.004          1      0.012     0.0069\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/20      2.08G      1.045      2.425     0.9934         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.7it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 6.7it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0121    0.00701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/20      2.08G      1.037      2.415      1.005         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.4it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.8it/s 0.1s\n",
            "                   all         15         18      0.004          1      0.012    0.00668\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/20       2.1G     0.9223      2.192     0.9753         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.8it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.1it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0119    0.00693\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/20      2.12G     0.8347      2.229     0.8991          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.2it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.2it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0117    0.00689\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/20      2.12G      0.938      2.106      1.021         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.9it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.4it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0116    0.00695\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/20      2.12G     0.8492      1.986     0.9153          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.9it/s 0.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.7it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0115    0.00707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/20      2.13G     0.8599      1.957     0.9437         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.7it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 7.9it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0114     0.0073\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/20      2.13G     0.8867      1.912     0.9768         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.7it/s 0.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 5.9it/s 0.2s\n",
            "                   all         15         18      0.004          1     0.0116    0.00743\n",
            "\n",
            "20 epochs completed in 0.009 hours.\n",
            "Optimizer stripped from /content/runs/exp/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/exp/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/exp/weights/best.pt...\n",
            "Ultralytics 8.3.218 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 9.9it/s 0.1s\n",
            "                   all         15         18      0.004          1     0.0138    0.00851\n",
            "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# @title ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "from ultralytics import YOLO\n",
        "#Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "#ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (yolov8n â†’ yolov8m â†’ yolov8l)\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ\n",
        "results = model.train(\n",
        "    data='/content/drive/MyDrive/custom_yolo/runs/yolov8_custom.yaml',  # ĞŸÑƒÑ‚ÑŒ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°\n",
        "    epochs=20,                 # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ…\n",
        "    imgsz=[640, 352],          # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "    batch=16,                   # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ°\n",
        "    lr0=0.01,                   # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ\n",
        "    project='runs',             # Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²\n",
        "    name='exp',                 # Ğ˜Ğ¼Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°\n",
        "    exist_ok=True               # Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑŒ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZXZ7WpGnZ4u"
      },
      "outputs": [],
      "source": [
        "# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
        "model = YOLO('/content/runs/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYr0j1k1nbsW",
        "outputId": "032fdad3-d14b-402f-9609-87d7c41d73f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/custom_yolo/runs/data/test/images/1b170f24-frame_23.jpg: 352x640 1 preform, 89.8ms\n",
            "Speed: 1.7ms preprocess, 89.8ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/custom_yolo/runs/data/test/images/e0717e7e-frame_188.jpg: 352x640 1 preform, 89.1ms\n",
            "Speed: 1.7ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/custom_yolo/runs/data/test/images/9a0ee140-frame_73.jpg: 352x640 (no detections), 92.4ms\n",
            "Speed: 1.8ms preprocess, 92.4ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n"
          ]
        }
      ],
      "source": [
        "# Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸\n",
        "results = model.predict('/content/drive/MyDrive/custom_yolo/runs/data/test/images/1b170f24-frame_23.jpg')\n",
        "results = model.predict('/content/drive/MyDrive/custom_yolo/runs/data/test/images/e0717e7e-frame_188.jpg')\n",
        "results = model.predict('/content/drive/MyDrive/custom_yolo/runs/data/test/images/9a0ee140-frame_73.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLJhACK-nskZ"
      },
      "outputs": [],
      "source": [
        "def detect_objects(img):\n",
        "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "    #img = cv2.imread(image_path)\n",
        "\n",
        "    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ\n",
        "    results = model.predict(\n",
        "      img,\n",
        "      conf=0.25,  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ\n",
        "      iou=0.45,   # ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ\n",
        "      max_det=1000  # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹\n",
        "    )\n",
        "\n",
        "    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²\n",
        "    for result in results:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ bounding box\n",
        "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "\n",
        "            # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ° Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸\n",
        "            cls = int(box.cls[0])\n",
        "            conf = float(box.conf[0])\n",
        "\n",
        "            # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ°\n",
        "            class_name = model.names[cls]\n",
        "\n",
        "            # Ğ Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ bounding box\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ°\n",
        "            cv2.putText(img, f'{class_name} {conf:.2f}', (int(x1), int(y1 - 10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dRym2Xdmnwlt"
      },
      "outputs": [],
      "source": [
        "# @title Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸\n",
        "CONFIDENCE_THRESHOLD = 0.6\n",
        "INPUT_WIDTH = 640\n",
        "INPUT_HEIGHT = 352\n",
        "# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹\n",
        "#cap = cv2.VideoCapture(0)  # 0 - Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ°\n",
        "path_video = '/content/drive/MyDrive/track001.mp4'\n",
        "# Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ½Ğ° Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ°Ğ¹Ğ»Ñƒ - Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "cap = cv2.VideoCapture(path_video)\n",
        "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "# Ğ¦Ğ²ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸\n",
        "colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0)]\n",
        "ROI_number = 0\n",
        "for i in range(length):\n",
        "#while True:\n",
        "    # Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ° Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² ĞºĞ°Ğ´Ñ€Ğ°\n",
        "    (H, W) = frame.shape[:2]\n",
        "    # ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ\n",
        "    # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ\n",
        "    image = frame#cv2.resize(frame, (INPUT_WIDTH, INPUT_HEIGHT))\n",
        "    #image = image / 255.0\n",
        "    #image = np.expand_dims(image, axis=0)\n",
        "    # Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²\n",
        "    detected_image = detect_objects(image)\n",
        "    # ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°\n",
        "    cv2_imshow(detected_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "    # Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ Ğ½Ğ°Ğ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ»Ğ°Ğ²Ğ¸ÑˆĞ¸ 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "# ĞÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²\n",
        "cap.release()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
